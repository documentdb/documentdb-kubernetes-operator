name: Test - Backup and Restore

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      node_count:
        description: 'Number of DocumentDB nodes'
        required: false
        default: '1'
      image_tag:
        description: 'Optional: Use existing image tag instead of building locally'
        required: false
        type: string
  workflow_call:
    inputs:
      image_tag:
        description: 'Optional: Use existing image tag instead of building locally'
        required: false
        type: string
      node_count:
        description: 'Number of DocumentDB nodes'
        required: false
        default: '1'
        type: string

permissions:
  contents: read
  actions: read
  packages: read

env:
  CERT_MANAGER_NS: cert-manager
  OPERATOR_NS: documentdb-operator
  DB_NS: documentdb-backup-and-restore-test
  DB_NAME: documentdb-backup-and-restore
  DB_RESTORE_NAME: documentdb-restore-from-backup
  DB_USERNAME: k8s_secret_user
  DB_PASSWORD: K8sSecret100
  DB_PORT: 10260
  DOCUMENTDB_IMAGE: ghcr.io/guanzhousongmicrosoft/documentdb-pg18:0.110.0

jobs:
  # Conditional build workflow - only run if image_tag is not provided or on pull_request
  build:
    name: Build Images and Charts
    if: ${{ (inputs.image_tag == '' || inputs.image_tag == null) || github.event_name == 'pull_request' }}
    uses: ./.github/workflows/test-build-and-package.yml
    with:
      version: '0.1.0'
    secrets: inherit
    
  backup-and-restore-test:
    name: Run Backup and Restore Tests
    runs-on: ${{ matrix.runner }}
    timeout-minutes: 60
    needs: build
    if: always() && (needs.build.result == 'success' || needs.build.result == 'skipped')

    strategy:
      matrix:
        include:
          - architecture: amd64
            runner: ubuntu-22.04
            test_scenario_name: "single-node"
            node_count: 1
            instances_per_node: 1
          - architecture: arm64
            runner: ubuntu-22.04-arm
            test_scenario_name: "single-node"
            node_count: 1
            instances_per_node: 1
    env:
      # Use built image tag on PR or when no external tag provided
      IMAGE_TAG: ${{ (github.event_name == 'pull_request' || inputs.image_tag == '' || inputs.image_tag == null) && needs.build.outputs.image_tag || inputs.image_tag }}
      CHART_VERSION: ${{ needs.build.outputs.chart_version || '0.1.0' }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Download artifacts
      if: ${{ (inputs.image_tag == '' || inputs.image_tag == null) || github.event_name == 'pull_request' }}
      uses: actions/download-artifact@v4
      with:
        pattern: 'build-*'
        path: ./artifacts

    - name: Log test configuration
      run: |
        echo "## Backup and Restore Test Configuration" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if [[ -n "${{ inputs.image_tag }}" ]]; then
          echo "- **Mode**: Using provided image tag" >> $GITHUB_STEP_SUMMARY
          echo "- **Image Tag**: \`${{ inputs.image_tag }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Source**: External (no local build)" >> $GITHUB_STEP_SUMMARY
        else
          echo "- **Mode**: Using locally built images" >> $GITHUB_STEP_SUMMARY
          echo "- **Image Tag**: \`${{ env.IMAGE_TAG }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Source**: Local build pipeline" >> $GITHUB_STEP_SUMMARY
        fi
        echo "- **Architecture**: \`${{ matrix.architecture }}\`" >> $GITHUB_STEP_SUMMARY

    - name: Setup test environment
      uses: ./.github/actions/setup-test-environment
      with:
        architecture: ${{ matrix.architecture }}
        runner: ${{ matrix.runner }}
        test-scenario-name: ${{ matrix.test_scenario_name }}
        node-count: '${{ matrix.node_count }}'
        instances-per-node: '${{ matrix.instances_per_node }}'
        cert-manager-namespace: ${{ env.CERT_MANAGER_NS }}
        operator-namespace: ${{ env.OPERATOR_NS }}
        db-namespace: ${{ env.DB_NS }}
        db-cluster-name: ${{ env.DB_NAME }}
        db-username: ${{ env.DB_USERNAME }}
        db-password: ${{ env.DB_PASSWORD }}
        db-port: ${{ env.DB_PORT }}
        image-tag: ${{ env.IMAGE_TAG }}
        chart-version: ${{ env.CHART_VERSION }}
        documentdb-image: ${{ env.DOCUMENTDB_IMAGE }}
        use-external-images: ${{ github.event_name != 'pull_request' && inputs.image_tag != '' && inputs.image_tag != null }}
        github-token: ${{ secrets.GITHUB_TOKEN }}
        repository-owner: ${{ github.repository_owner }}
        kubernetes-version: 'v1.35.0'
    - name: Setup port forwarding for comprehensive tests
      uses: ./.github/actions/setup-port-forwarding
      with:
        namespace: ${{ env.DB_NS }}
        cluster-name: ${{ env.DB_NAME }}
        port: ${{ env.DB_PORT }}
        architecture: ${{ matrix.architecture }}
        test-type: 'comprehensive'

    - name: Insert test data using mongosh
      run: |
        echo "Inserting test data into DocumentDB cluster..."
        if mongosh 127.0.0.1:$DB_PORT \
          -u $DB_USERNAME \
          -p $DB_PASSWORD \
          --authenticationMechanism SCRAM-SHA-256 \
          --tls \
          --tlsAllowInvalidCertificates \
          --eval "for (let i = 1; i <= 100; i++) { db.testCollection.insertOne({ index: i, message: 'This is document ' + i }); }" ; then
          echo "✓ Test data insertion completed successfully on ${{ matrix.architecture }}"
        else
          echo "❌ Test data insertion failed on ${{ matrix.architecture }}"
          exit 1
        fi

        echo "Verifying inserted test data..."
        count=$(mongosh 127.0.0.1:$DB_PORT --quiet --eval "db.testCollection.countDocuments({})" -u $DB_USERNAME -p $DB_PASSWORD --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates)
        if [[ "$count" -eq 100 ]]; then
          echo "✓ Test data verification completed successfully on ${{ matrix.architecture }}"
        else
          echo "❌ Test data verification failed on ${{ matrix.architecture }}"
          exit 1
        fi

    - name: Create ScheduledBackup to trigger backups
      shell: bash
      run: |
        cat <<EOF | kubectl apply -f -
        apiVersion: documentdb.io/preview
        kind: ScheduledBackup
        metadata:
          name: $DB_NAME-backup
          namespace: $DB_NS
        spec:
          cluster:
            name: $DB_NAME
          # every one minute
          schedule: "*/1 * * * *"
        EOF

        schedule_status=$(kubectl -n $DB_NS get scheduledbackups $DB_NAME-backup -o jsonpath='{.metadata.name}')
        if [ "$schedule_status" == "$DB_NAME-backup" ]; then
          echo "✓ ScheduledBackup created successfully."
        else
          echo "❌ Failed to create ScheduledBackup."
          exit 1
        fi

    - name: Wait for backup to complete
      shell: bash
      run: |
        echo "Waiting for backup to complete..."
        MAX_RETRIES=15
        SLEEP_INTERVAL=20
        ITER=0
        while [ $ITER -lt $MAX_RETRIES ]; do
          # check if there is at least two backups with phase completed
          backup_status=$(kubectl -n $DB_NS get backups -o jsonpath='{.items[?(@.status.phase=="completed")].metadata.name}' | wc -w)
          if [ "$backup_status" -ge 2 ]; then
            echo "✓ Backup completed successfully."
            exit 0
          else
            echo "Current backup status: $backup_status."
            kubectl -n $DB_NS get backups

            echo "Retrying in $SLEEP_INTERVAL seconds..."
            sleep $SLEEP_INTERVAL
          fi
          ((++ITER))
        done
        echo "❌ Backup did not complete within expected time."
        
        echo "Fetching operator logs for debugging..."
        operator_name=$(kubectl get pods -A | grep -E 'documentdb-operator-' | grep -v 'documentdb-operator-cloudnative-pg-' | awk '{print $2}')
        kubectl -n $OPERATOR_NS logs $operator_name | grep 'Backup' || echo "No Backup related logs found."
        
        exit 1

    - name: Delete test data before restore
      run: |
        echo "Deleting test data from DocumentDB cluster before restore..."
        if mongosh 127.0.0.1:$DB_PORT \
          -u $DB_USERNAME \
          -p $DB_PASSWORD \
          --authenticationMechanism SCRAM-SHA-256 \
          --tls \
          --tlsAllowInvalidCertificates \
          --eval "db.testCollection.deleteMany({});" ; then
          echo "✓ Test data deletion completed successfully on ${{ matrix.architecture }}"
        else
          echo "❌ Test data deletion failed on ${{ matrix.architecture }}"
          exit 1
        fi

        count=$(mongosh 127.0.0.1:$DB_PORT --quiet --eval "db.testCollection.countDocuments({})" -u $DB_USERNAME -p $DB_PASSWORD --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates)
        if [[ "$count" -eq 0 ]]; then
          echo "✓ Test data deletion verified successfully on ${{ matrix.architecture }}"
        else
          echo "❌ Test data deletion verification failed on ${{ matrix.architecture }}"
          exit 1
        fi

    - name: Cleanup comprehensive test port forwarding
      if: always()
      run: |
        # Stop port-forward if it exists
        if [ -f /tmp/pf_pid ]; then
          PF_PID=$(cat /tmp/pf_pid)
          kill $PF_PID 2>/dev/null || true
          rm -f /tmp/pf_pid
        fi
        
        # Clean up output log
        rm -f /tmp/pf_output.log
        
        # Clean up output log
        rm -f /tmp/pf_output.log

    - name: Restore from backup
      shell: bash
      run: |
        # Get the latest backup name
        backup_name=$(kubectl -n ${{ env.DB_NS }} get backups -o jsonpath='{.items[?(@.status.phase=="completed")].metadata.name}' | tr ' ' '\n' | sort | tail -n 1)

        # Create DocumentDB resource
        cat <<EOF | kubectl apply -f -
        apiVersion: documentdb.io/preview
        kind: DocumentDB
        metadata:
          name: ${{ env.DB_RESTORE_NAME }}
          namespace: ${{ env.DB_NS }}
        spec:
          nodeCount: ${{ matrix.node_count }}
          instancesPerNode: ${{ matrix.instances_per_node }}
          documentDBImage: ${{ env.DOCUMENTDB_IMAGE }}
          gatewayImage: ghcr.io/microsoft/documentdb/documentdb-local:16
          resource:
            storage:
              pvcSize: 5Gi
              storageClass: csi-hostpath-sc
          exposeViaService:
            serviceType: ClusterIP
          bootstrap:
            recovery:
              backup:
                name: $backup_name
        EOF

    - name: Wait for restored DocumentDB cluster to become healthy
      shell: bash
      run: |
        echo "Waiting for restored DocumentDB cluster to become healthy..."
        MAX_RETRIES=60
        SLEEP_INTERVAL=10
        ITER=0
        while [ $ITER -lt $MAX_RETRIES ]; do
          status=$(kubectl -n ${{ env.DB_NS }} get documentdb ${{ env.DB_RESTORE_NAME }} -o jsonpath='{.status.status}' 2>/dev/null || echo "")
          if [ "$status" == "Cluster in healthy state" ]; then
            echo "✓ DocumentDB cluster ${{ env.DB_RESTORE_NAME }} is healthy."
            exit 0
          else
            echo "Current status: $status. Waiting..."
            kubectl -n ${{ env.DB_NS }} get documentdb ${{ env.DB_RESTORE_NAME }} || true
            sleep $SLEEP_INTERVAL
          fi
          ((++ITER))
        done
        echo "❌ DocumentDB cluster ${{ env.DB_RESTORE_NAME }} did not become healthy within expected time."
        kubectl -n ${{ env.DB_NS }} describe documentdb ${{ env.DB_RESTORE_NAME }} || true
        kubectl -n ${{ env.DB_NS }} get pods -l cnpg.io/cluster=${{ env.DB_RESTORE_NAME }} || true
        exit 1

    - name: Setup port forwarding for comprehensive tests
      uses: ./.github/actions/setup-port-forwarding
      with:
        namespace: ${{ env.DB_NS }}
        cluster-name: ${{ env.DB_RESTORE_NAME }}
        port: ${{ env.DB_PORT }}
        architecture: ${{ matrix.architecture }}
        test-type: 'comprehensive'

    - name: Validate restored data
      run: |
        # Validate that the restored cluster has the expected data
        count=$(mongosh 127.0.0.1:$DB_PORT --quiet --eval "db.testCollection.countDocuments({})" -u $DB_USERNAME -p $DB_PASSWORD --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates)
        if [ "$count" -eq 100 ]; then
          echo "✓ Data validation completed successfully on ${{ matrix.architecture }}"
        else
          echo "❌ Data validation failed on ${{ matrix.architecture }}"
          exit 1
        fi

    - name: Cleanup comprehensive test port forwarding
      if: always()
      run: |
        # Stop port-forward if it exists
        if [ -f /tmp/pf_pid ]; then
          PF_PID=$(cat /tmp/pf_pid)
          kill $PF_PID 2>/dev/null || true
          rm -f /tmp/pf_pid
        fi
        
        # Clean up output log
        rm -f /tmp/pf_output.log
        
        # Clean up output log
        rm -f /tmp/pf_output.log
    
    - name: Test if expired backups are cleaned up
      shell: bash
      run: |
        echo "Verifying expired backups are cleaned up..."
        # pick up one backup name
        backup_name=$(kubectl -n $DB_NS get backups -o jsonpath='{.items[0].metadata.name}')
        # set expiration time to past
        kubectl -n $DB_NS patch backup $backup_name --type='json' --type=merge -p='{"status":{"expiredAt":"2000-01-01T00:00:00Z"}}' --subresource=status
        # wait for cleanup
        MAX_RETRIES=10
        SLEEP_INTERVAL=15
        ITER=0
        while [ $ITER -lt $MAX_RETRIES ]; do
          backup_status=$(kubectl -n $DB_NS get backup $backup_name --ignore-not-found)
          if [ -z "$backup_status" ]; then
            echo "✓ Expired backup cleaned up successfully."
            exit 0
          else
            echo "Backup $backup_name still exists. Retrying in $SLEEP_INTERVAL seconds..."
            kubectl -n $DB_NS get backup $backup_name
            sleep $SLEEP_INTERVAL
          fi
          ((++ITER))
        done
        echo "❌ Expired backup was not cleaned up within expected time."
        exit 1  
     
    - name: Test PV retention after DocumentDB deletion
      id: test-pv-retention
      shell: bash
      run: |
        echo "Testing PV retention after DocumentDB deletion..."
        
        # Find PV directly using documentdb.io labels set by the PV controller
        # PVs are cluster-scoped and labeled with documentdb.io/cluster and documentdb.io/namespace
        pv_name=$(kubectl get pv -l documentdb.io/cluster=${{ env.DB_RESTORE_NAME }},documentdb.io/namespace=${{ env.DB_NS }} -o jsonpath='{.items[0].metadata.name}')
        echo "PV name: $pv_name"
        
        if [ -z "$pv_name" ]; then
          echo "❌ Failed to find PV with documentdb.io/cluster=${{ env.DB_RESTORE_NAME }} and documentdb.io/namespace=${{ env.DB_NS }}"
          exit 1
        fi
        
        # Check current PV reclaim policy - should be Retain by default
        current_policy=$(kubectl get pv $pv_name -o jsonpath='{.spec.persistentVolumeReclaimPolicy}')
        echo "Current PV reclaim policy: $current_policy"
        
        if [ "$current_policy" != "Retain" ]; then
          echo "❌ Expected PV reclaim policy to be 'Retain' (default), but got '$current_policy'"
          exit 1
        fi
        echo "✓ PV reclaim policy is correctly set to Retain (default)"
        
        # Delete the restored DocumentDB cluster
        kubectl -n ${{ env.DB_NS }} delete documentdb ${{ env.DB_RESTORE_NAME }} --wait=false
        
        # Wait for DocumentDB to be deleted
        echo "Waiting for DocumentDB to be deleted..."
        MAX_RETRIES=30
        SLEEP_INTERVAL=10
        ITER=0
        while [ $ITER -lt $MAX_RETRIES ]; do
          db_exists=$(kubectl -n ${{ env.DB_NS }} get documentdb ${{ env.DB_RESTORE_NAME }} --ignore-not-found)
          if [ -z "$db_exists" ]; then
            echo "✓ DocumentDB deleted successfully."
            break
          else
            echo "DocumentDB still exists. Waiting..."
            sleep $SLEEP_INTERVAL
          fi
          ((++ITER))
        done
        
        # Verify PV still exists (because reclaim policy is Retain)
        pv_exists=$(kubectl get pv $pv_name --ignore-not-found)
        if [ -n "$pv_exists" ]; then
          echo "✓ PV $pv_name retained after DocumentDB deletion"
        else
          echo "❌ PV $pv_name was deleted unexpectedly"
          exit 1
        fi
        
        # Verify pv_name is not empty before writing to GITHUB_OUTPUT
        if [ -z "$pv_name" ]; then
          echo "❌ Error: PV name is empty, cannot proceed with recovery test"
          exit 1
        fi
        
        # Store PV name for later steps using GitHub Actions output (more robust than temp files)
        echo "pv_name=$pv_name" >> $GITHUB_OUTPUT

    - name: Restore DocumentDB from retained PV
      shell: bash
      run: |
        pv_name="${{ steps.test-pv-retention.outputs.pv_name }}"
        echo "Restoring DocumentDB from retained PV: $pv_name"
        
        # Create DocumentDB resource with PV recovery
        echo "Creating DocumentDB with PV recovery from $pv_name"
        cat <<EOF | kubectl apply -f -
        apiVersion: documentdb.io/preview
        kind: DocumentDB
        metadata:
          name: ${{ env.DB_RESTORE_NAME }}-from-pv
          namespace: ${{ env.DB_NS }}
        spec:
          nodeCount: ${{ matrix.node_count }}
          instancesPerNode: ${{ matrix.instances_per_node }}
          documentDBImage: ghcr.io/microsoft/documentdb/documentdb-local:16
          gatewayImage: ghcr.io/microsoft/documentdb/documentdb-local:16
          resource:
            storage:
              pvcSize: 5Gi
              storageClass: csi-hostpath-sc
          exposeViaService:
            serviceType: ClusterIP
          bootstrap:
            recovery:
              persistentVolume:
                name: $pv_name
        EOF

    - name: Wait for PV restored DocumentDB cluster to become healthy
      shell: bash
      run: |
        echo "Waiting for PV restored DocumentDB cluster to become healthy..."
        MAX_RETRIES=60
        SLEEP_INTERVAL=10
        ITER=0
        while [ $ITER -lt $MAX_RETRIES ]; do
          status=$(kubectl -n ${{ env.DB_NS }} get documentdb ${{ env.DB_RESTORE_NAME }}-from-pv -o jsonpath='{.status.status}' 2>/dev/null || echo "")
          if [ "$status" == "Cluster in healthy state" ]; then
            echo "✓ DocumentDB cluster ${{ env.DB_RESTORE_NAME }}-from-pv is healthy."
            exit 0
          else
            echo "Current status: $status. Waiting..."
            kubectl -n ${{ env.DB_NS }} get documentdb ${{ env.DB_RESTORE_NAME }}-from-pv || true
            sleep $SLEEP_INTERVAL
          fi
          ((++ITER))
        done
        echo "❌ DocumentDB cluster ${{ env.DB_RESTORE_NAME }}-from-pv did not become healthy within expected time."
        kubectl -n ${{ env.DB_NS }} describe documentdb ${{ env.DB_RESTORE_NAME }}-from-pv || true
        kubectl -n ${{ env.DB_NS }} get pods -l cnpg.io/cluster=${{ env.DB_RESTORE_NAME }}-from-pv || true
        exit 1

    - name: Setup port forwarding for PV restored cluster
      uses: ./.github/actions/setup-port-forwarding
      with:
        namespace: ${{ env.DB_NS }}
        cluster-name: ${{ env.DB_RESTORE_NAME }}-from-pv
        port: ${{ env.DB_PORT }}
        architecture: ${{ matrix.architecture }}
        test-type: 'comprehensive'

    - name: Validate data exists after PV restoration
      run: |
        echo "Validating data exists after PV restoration..."
        
        # Validate that the restored cluster has the expected data
        count=$(mongosh 127.0.0.1:$DB_PORT --quiet --eval "db.testCollection.countDocuments({})" -u $DB_USERNAME -p $DB_PASSWORD --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates)
        if [ "$count" -eq 100 ]; then
          echo "✓ Data validation completed successfully after PV restoration on ${{ matrix.architecture }}"
        else
          echo "❌ Data validation failed after PV restoration on ${{ matrix.architecture }}. Count: $count"
          exit 1
        fi

    - name: Verify temporary recovery PVC is cleaned up
      shell: bash
      run: |
        echo "Verifying temporary recovery PVC is cleaned up after cluster is healthy..."

        # The temp PVC name follows the pattern: <documentdbName>-pv-recovery-temp
        temp_pvc_name="${{ env.DB_RESTORE_NAME }}-from-pv-pv-recovery-temp"
        
        # Wait a bit for cleanup to happen
        sleep 10
        
        # Check if temporary PVC still exists
        temp_pvc_exists=$(kubectl -n ${{ env.DB_NS }} get pvc $temp_pvc_name --ignore-not-found -o name)
        if [ -z "$temp_pvc_exists" ]; then
          echo "✓ Temporary recovery PVC $temp_pvc_name was cleaned up successfully"
        else
          echo "❌ Temporary recovery PVC $temp_pvc_name still exists after cluster is healthy"
          kubectl -n ${{ env.DB_NS }} get pvc $temp_pvc_name
          exit 1
        fi

    - name: Cleanup PV restored cluster port forwarding
      if: always()
      run: |
        # Stop port-forward if it exists
        if [ -f /tmp/pf_pid ]; then
          PF_PID=$(cat /tmp/pf_pid)
          kill $PF_PID 2>/dev/null || true
          rm -f /tmp/pf_pid
        fi
        
        # Clean up output log
        rm -f /tmp/pf_output.log

    - name: Collect logs on failure
      if: failure()
      uses: ./.github/actions/collect-logs
      with:
        architecture: ${{ matrix.architecture }}
        operator-namespace: ${{ env.OPERATOR_NS }}
        db-namespace: ${{ env.DB_NS }}
        db-cluster-name: ${{ env.DB_NAME }}
        cert-manager-namespace: ${{ env.CERT_MANAGER_NS }}
