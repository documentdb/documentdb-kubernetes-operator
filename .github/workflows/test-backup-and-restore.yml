name: Test - Backup and Restore

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      documentdb_version:
        description: 'DocumentDB image version to test'
        required: false
        default: '16'
      node_count:
        description: 'Number of DocumentDB nodes'
        required: false
        default: '1'
      image_tag:
        description: 'Optional: Use existing image tag instead of building locally'
        required: false
        type: string
  workflow_call:
    inputs:
      image_tag:
        description: 'Optional: Use existing image tag instead of building locally'
        required: false
        type: string
      documentdb_version:
        description: 'DocumentDB image version to test'
        required: false
        default: '16'
        type: string
      node_count:
        description: 'Number of DocumentDB nodes'
        required: false
        default: '1'
        type: string

permissions:
  contents: read
  actions: read
  packages: read

env:
  CERT_MANAGER_NS: cert-manager
  OPERATOR_NS: documentdb-operator
  DB_NS: documentdb-backup-and-restore-test
  DB_NAME: documentdb-backup-and-restore
  DB_RESTORE_NAME: documentdb-restore-from-backup
  DB_USERNAME: k8s_secret_user
  DB_PASSWORD: K8sSecret100
  DB_PORT: 10260

jobs:
  # Conditional build workflow - only run if image_tag is not provided or on pull_request
  build:
    name: Build Images and Charts
    if: ${{ (inputs.image_tag == '' || inputs.image_tag == null) || github.event_name == 'pull_request' }}
    uses: ./.github/workflows/test-build-and-package.yml
    with:
      version: '0.1.0'
    secrets: inherit
    
  backup-and-restore-test:
    name: Run Backup and Restore Tests
    runs-on: ${{ matrix.runner }}
    timeout-minutes: 60
    needs: build
    if: always() && (needs.build.result == 'success' || needs.build.result == 'skipped')

    strategy:
      matrix:
        include:
          - architecture: amd64
            runner: ubuntu-22.04
            test_scenario_name: "single-node"
            node_count: 1
            instances_per_node: 1
          - architecture: arm64
            runner: ubuntu-22.04-arm
            test_scenario_name: "single-node"
            node_count: 1
            instances_per_node: 1
    env:
      # Use built image tag on PR or when no external tag provided
      IMAGE_TAG: ${{ (github.event_name == 'pull_request' || inputs.image_tag == '' || inputs.image_tag == null) && needs.build.outputs.image_tag || inputs.image_tag }}
      CHART_VERSION: ${{ needs.build.outputs.chart_version || '0.1.0' }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Download artifacts
      if: ${{ (inputs.image_tag == '' || inputs.image_tag == null) || github.event_name == 'pull_request' }}
      uses: actions/download-artifact@v4
      with:
        pattern: 'build-*'
        path: ./artifacts

    - name: Log test configuration
      run: |
        echo "## Backup and Restore Test Configuration" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if [[ -n "${{ inputs.image_tag }}" ]]; then
          echo "- **Mode**: Using provided image tag" >> $GITHUB_STEP_SUMMARY
          echo "- **Image Tag**: \`${{ inputs.image_tag }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Source**: External (no local build)" >> $GITHUB_STEP_SUMMARY
        else
          echo "- **Mode**: Using locally built images" >> $GITHUB_STEP_SUMMARY
          echo "- **Image Tag**: \`${{ env.IMAGE_TAG }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Source**: Local build pipeline" >> $GITHUB_STEP_SUMMARY
        fi
        echo "- **Architecture**: \`${{ matrix.architecture }}\`" >> $GITHUB_STEP_SUMMARY

    - name: Setup test environment
      uses: ./.github/actions/setup-test-environment
      with:
        architecture: ${{ matrix.architecture }}
        runner: ${{ matrix.runner }}
        test-scenario-name: ${{ matrix.test_scenario_name }}
        node-count: '${{ matrix.node_count }}'
        instances-per-node: '${{ matrix.instances_per_node }}'
        cert-manager-namespace: ${{ env.CERT_MANAGER_NS }}
        operator-namespace: ${{ env.OPERATOR_NS }}
        db-namespace: ${{ env.DB_NS }}
        db-cluster-name: ${{ env.DB_NAME }}
        db-username: ${{ env.DB_USERNAME }}
        db-password: ${{ env.DB_PASSWORD }}
        db-port: ${{ env.DB_PORT }}
        image-tag: ${{ env.IMAGE_TAG }}
        chart-version: ${{ env.CHART_VERSION }}
        use-external-images: ${{ github.event_name != 'pull_request' && inputs.image_tag != '' && inputs.image_tag != null }}
        github-token: ${{ secrets.GITHUB_TOKEN }}
        repository-owner: ${{ github.repository_owner }}

    - name: Setup port forwarding for comprehensive tests
      uses: ./.github/actions/setup-port-forwarding
      with:
        namespace: ${{ env.DB_NS }}
        cluster-name: ${{ env.DB_NAME }}
        port: ${{ env.DB_PORT }}
        architecture: ${{ matrix.architecture }}
        test-type: 'comprehensive'

    - name: Insert test data using mongosh
      run: |
        echo "Inserting test data into DocumentDB cluster..."
        if mongosh 127.0.0.1:$DB_PORT \
          -u $DB_USERNAME \
          -p $DB_PASSWORD \
          --authenticationMechanism SCRAM-SHA-256 \
          --tls \
          --tlsAllowInvalidCertificates \
          --eval "for (let i = 1; i <= 100; i++) { db.testCollection.insertOne({ index: i, message: 'This is document ' + i }); }" ; then
          echo "✓ Test data insertion completed successfully on ${{ matrix.architecture }}"
        else
          echo "❌ Test data insertion failed on ${{ matrix.architecture }}"
          exit 1
        fi

        echo "Verifying inserted test data..."
        count=$(mongosh 127.0.0.1:$DB_PORT --quiet --eval "db.testCollection.countDocuments({})" -u $DB_USERNAME -p $DB_PASSWORD --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates)
        if [[ "$count" -eq 100 ]]; then
          echo "✓ Test data verification completed successfully on ${{ matrix.architecture }}"
        else
          echo "❌ Test data verification failed on ${{ matrix.architecture }}"
          exit 1
        fi

    - name: Create ScheduledBackup to trigger backups
      shell: bash
      run: |
        cat <<EOF | kubectl apply -f -
        apiVersion: documentdb.io/preview
        kind: ScheduledBackup
        metadata:
          name: $DB_NAME-backup
          namespace: $DB_NS
        spec:
          cluster:
            name: $DB_NAME
          # every one minute
          schedule: "*/1 * * * *"
        EOF

        schedule_status=$(kubectl -n $DB_NS get scheduledbackups $DB_NAME-backup -o jsonpath='{.metadata.name}')
        if [ "$schedule_status" == "$DB_NAME-backup" ]; then
          echo "✓ ScheduledBackup created successfully."
        else
          echo "❌ Failed to create ScheduledBackup."
          exit 1
        fi

    - name: Wait for backup to complete
      shell: bash
      run: |
        echo "Waiting for backup to complete..."
        MAX_RETRIES=15
        SLEEP_INTERVAL=20
        ITER=0
        while [ $ITER -lt $MAX_RETRIES ]; do
          # check if there is at least two backups with phase completed
          backup_status=$(kubectl -n $DB_NS get backups -o jsonpath='{.items[?(@.status.phase=="completed")].metadata.name}' | wc -w)
          if [ "$backup_status" -ge 2 ]; then
            echo "✓ Backup completed successfully."
            exit 0
          else
            echo "Current backup status: $backup_status."
            kubectl -n $DB_NS get backups

            echo "Retrying in $SLEEP_INTERVAL seconds..."
            sleep $SLEEP_INTERVAL
          fi
          ((++ITER))
        done
        echo "❌ Backup did not complete within expected time."
        
        echo "Fetching operator logs for debugging..."
        operator_name=$(kubectl get pods -A | grep -E 'documentdb-operator-' | grep -v 'documentdb-operator-cloudnative-pg-' | awk '{print $2}')
        kubectl -n $OPERATOR_NS logs $operator_name | grep 'Backup' || echo "No Backup related logs found."
        
        exit 1

    - name: Delete test data before restore
      run: |
        echo "Deleting test data from DocumentDB cluster before restore..."
        if mongosh 127.0.0.1:$DB_PORT \
          -u $DB_USERNAME \
          -p $DB_PASSWORD \
          --authenticationMechanism SCRAM-SHA-256 \
          --tls \
          --tlsAllowInvalidCertificates \
          --eval "db.testCollection.deleteMany({});" ; then
          echo "✓ Test data deletion completed successfully on ${{ matrix.architecture }}"
        else
          echo "❌ Test data deletion failed on ${{ matrix.architecture }}"
          exit 1
        fi

        count=$(mongosh 127.0.0.1:$DB_PORT --quiet --eval "db.testCollection.countDocuments({})" -u $DB_USERNAME -p $DB_PASSWORD --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates)
        if [[ "$count" -eq 0 ]]; then
          echo "✓ Test data deletion verified successfully on ${{ matrix.architecture }}"
        else
          echo "❌ Test data deletion verification failed on ${{ matrix.architecture }}"
          exit 1
        fi

    - name: Cleanup comprehensive test port forwarding
      if: always()
      run: |
        # Stop port-forward if it exists
        if [ -f /tmp/pf_pid ]; then
          PF_PID=$(cat /tmp/pf_pid)
          kill $PF_PID 2>/dev/null || true
          rm -f /tmp/pf_pid
        fi
        
        # Clean up output log
        rm -f /tmp/pf_output.log
        
        # Clean up output log
        rm -f /tmp/pf_output.log

    - name: Restore from backup
      shell: bash
      run: |
        # Get the latest backup name
        backup_name=$(kubectl -n ${{ env.DB_NS }} get backups -o jsonpath='{.items[?(@.status.phase=="completed")].metadata.name}' | tr ' ' '\n' | sort | tail -n 1)

        # Create DocumentDB resource
        cat <<EOF | kubectl apply -f -
        apiVersion: documentdb.io/preview
        kind: DocumentDB
        metadata:
          name: ${{ env.DB_RESTORE_NAME }}
          namespace: ${{ env.DB_NS }}
        spec:
          nodeCount: ${{ matrix.node_count }}
          instancesPerNode: ${{ matrix.instances_per_node }}
          documentDBImage: ghcr.io/microsoft/documentdb/documentdb-local:16
          gatewayImage: ghcr.io/microsoft/documentdb/documentdb-local:16
          resource:
            storage:
              pvcSize: 5Gi
              storageClass: csi-hostpath-sc
              persistentVolumeReclaimPolicy: Retain
          exposeViaService:
            serviceType: ClusterIP
          bootstrap:
            recovery:
              backup:
                name: $backup_name
        EOF

    - name: Setup port forwarding for comprehensive tests
      uses: ./.github/actions/setup-port-forwarding
      with:
        namespace: ${{ env.DB_NS }}
        cluster-name: ${{ env.DB_RESTORE_NAME }}
        port: ${{ env.DB_PORT }}
        architecture: ${{ matrix.architecture }}
        test-type: 'comprehensive'

    - name: Validate restored data
      run: |
        # Validate that the restored cluster has the expected data
        count=$(mongosh 127.0.0.1:$DB_PORT --quiet --eval "db.testCollection.countDocuments({})" -u $DB_USERNAME -p $DB_PASSWORD --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates)
        if [ "$count" -eq 100 ]; then
          echo "✓ Data validation completed successfully on ${{ matrix.architecture }}"
        else
          echo "❌ Data validation failed on ${{ matrix.architecture }}"
          exit 1
        fi

    - name: Cleanup comprehensive test port forwarding
      if: always()
      run: |
        # Stop port-forward if it exists
        if [ -f /tmp/pf_pid ]; then
          PF_PID=$(cat /tmp/pf_pid)
          kill $PF_PID 2>/dev/null || true
          rm -f /tmp/pf_pid
        fi
        
        # Clean up output log
        rm -f /tmp/pf_output.log
        
        # Clean up output log
        rm -f /tmp/pf_output.log
    
    - name: Test if expired backups are cleaned up
      shell: bash
      run: |
        echo "Verifying expired backups are cleaned up..."
        # pick up one backup name
        backup_name=$(kubectl -n $DB_NS get backups -o jsonpath='{.items[0].metadata.name}')
        # set expiration time to past
        kubectl -n $DB_NS patch backup $backup_name --type='json' --type=merge -p='{"status":{"expiredAt":"2000-01-01T00:00:00Z"}}' --subresource=status
        # wait for cleanup
        MAX_RETRIES=10
        SLEEP_INTERVAL=15
        ITER=0
        while [ $ITER -lt $MAX_RETRIES ]; do
          backup_status=$(kubectl -n $DB_NS get backup $backup_name --ignore-not-found)
          if [ -z "$backup_status" ]; then
            echo "✓ Expired backup cleaned up successfully."
            exit 0
          else
            echo "Backup $backup_name still exists. Retrying in $SLEEP_INTERVAL seconds..."
            kubectl -n $DB_NS get backup $backup_name
            sleep $SLEEP_INTERVAL
          fi
          ((++ITER))
        done
        echo "❌ Expired backup was not cleaned up within expected time."
        exit 1  
     
    - name: Test PV retention after DocumentDB deletion
      shell: bash
      run: |
        echo "Testing PV retention after DocumentDB deletion..."
        
        # Get the PVC name and PV name before deleting the DocumentDB
        pvc_name=$(kubectl -n ${{ env.DB_NS }} get pvc -l documentdb.io/cluster=${{ env.DB_RESTORE_NAME }} -o jsonpath='{.items[0].metadata.name}')
        echo "PVC name: $pvc_name"
        
        if [ -z "$pvc_name" ]; then
          echo "❌ Failed to find PVC for cluster ${{ env.DB_RESTORE_NAME }}"
          exit 1
        fi
        
        # Get the PV name bound to this PVC
        pv_name=$(kubectl -n ${{ env.DB_NS }} get pvc $pvc_name -o jsonpath='{.spec.volumeName}')
        echo "PV name: $pv_name"
        
        if [ -z "$pv_name" ]; then
          echo "❌ Failed to find PV bound to PVC $pvc_name"
          exit 1
        fi
        
        # Delete the restored DocumentDB cluster
        kubectl -n ${{ env.DB_NS }} delete documentdb ${{ env.DB_RESTORE_NAME }} --wait=false
        
        # Wait for DocumentDB to be deleted
        echo "Waiting for DocumentDB to be deleted..."
        MAX_RETRIES=30
        SLEEP_INTERVAL=10
        ITER=0
        while [ $ITER -lt $MAX_RETRIES ]; do
          db_exists=$(kubectl -n ${{ env.DB_NS }} get documentdb ${{ env.DB_RESTORE_NAME }} --ignore-not-found)
          if [ -z "$db_exists" ]; then
            echo "✓ DocumentDB deleted successfully."
            break
          else
            echo "DocumentDB still exists. Waiting..."
            sleep $SLEEP_INTERVAL
          fi
          ((++ITER))
        done
        
        # Verify PV still exists
        pv_exists=$(kubectl get pv $pv_name --ignore-not-found)
        if [ -n "$pv_exists" ]; then
          echo "✓ PV $pv_name retained after DocumentDB deletion"
        else
          echo "❌ PV $pv_name was deleted unexpectedly"
          exit 1
        fi
        
        # Store PV name for later steps
        echo "$pv_name" > /tmp/retained_pv_name

    - name: Restore DocumentDB from retained PV
      shell: bash
      run: |
        pv_name=$(cat /tmp/retained_pv_name)
        echo "Restoring DocumentDB from retained PV: $pv_name"
        
        # Check the PV status - it should be in "Released" state after PVC deletion
        pv_status=$(kubectl get pv $pv_name -o jsonpath='{.status.phase}')
        echo "PV status: $pv_status"
        
        # Clear the claimRef from the PV so a new PVC can bind to it
        # When a PV is in "Released" state, it still has a claimRef to the old deleted PVC
        echo "Clearing claimRef from PV $pv_name to allow new PVC binding..."
        kubectl patch pv $pv_name --type=json -p='[{"op": "remove", "path": "/spec/claimRef"}]'
        
        # Verify PV is now Available
        pv_status=$(kubectl get pv $pv_name -o jsonpath='{.status.phase}')
        echo "PV status after clearing claimRef: $pv_status"
        
        # Create a new PVC that binds to the retained PV
        new_pvc_name="recovered-pvc-from-pv"
        echo "Creating new PVC $new_pvc_name to bind to retained PV $pv_name"
        
        # Get the storage capacity from the PV
        pv_capacity=$(kubectl get pv $pv_name -o jsonpath='{.spec.capacity.storage}')
        echo "PV capacity: $pv_capacity"
        
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: $new_pvc_name
          namespace: ${{ env.DB_NS }}
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: $pv_capacity
          storageClassName: csi-hostpath-sc
          volumeName: $pv_name
        EOF
        
        # Wait for PVC to be bound
        echo "Waiting for PVC to be bound to PV..."
        MAX_RETRIES=30
        SLEEP_INTERVAL=5
        ITER=0
        while [ $ITER -lt $MAX_RETRIES ]; do
          pvc_status=$(kubectl -n ${{ env.DB_NS }} get pvc $new_pvc_name -o jsonpath='{.status.phase}')
          if [ "$pvc_status" == "Bound" ]; then
            echo "✓ PVC $new_pvc_name is now bound to PV $pv_name"
            break
          else
            echo "PVC status: $pvc_status. Waiting..."
            sleep $SLEEP_INTERVAL
          fi
          ((++ITER))
        done
        
        if [ "$pvc_status" != "Bound" ]; then
          echo "❌ PVC failed to bind to PV within expected time"
          exit 1
        fi
        
        # Create DocumentDB resource with PVC recovery
        echo "Creating DocumentDB with PVC recovery from $new_pvc_name"
        cat <<EOF | kubectl apply -f -
        apiVersion: documentdb.io/preview
        kind: DocumentDB
        metadata:
          name: ${{ env.DB_RESTORE_NAME }}-from-pvc
          namespace: ${{ env.DB_NS }}
        spec:
          nodeCount: ${{ matrix.node_count }}
          instancesPerNode: ${{ matrix.instances_per_node }}
          documentDBImage: ghcr.io/microsoft/documentdb/documentdb-local:16
          gatewayImage: ghcr.io/microsoft/documentdb/documentdb-local:16
          resource:
            storage:
              pvcSize: 5Gi
              storageClass: csi-hostpath-sc
          exposeViaService:
            serviceType: ClusterIP
          bootstrap:
            recovery:
              pvc:
                name: $new_pvc_name
        EOF

    - name: Setup port forwarding for PVC restored cluster
      uses: ./.github/actions/setup-port-forwarding
      with:
        namespace: ${{ env.DB_NS }}
        cluster-name: ${{ env.DB_RESTORE_NAME }}-from-pvc
        port: ${{ env.DB_PORT }}
        architecture: ${{ matrix.architecture }}
        test-type: 'comprehensive'

    - name: Validate data exists after PVC restoration
      run: |
        echo "Validating data exists after PVC restoration..."
        
        # Validate that the restored cluster has the expected data
        count=$(mongosh 127.0.0.1:$DB_PORT --quiet --eval "db.testCollection.countDocuments({})" -u $DB_USERNAME -p $DB_PASSWORD --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates)
        if [ "$count" -eq 100 ]; then
          echo "✓ Data validation completed successfully after PVC restoration on ${{ matrix.architecture }}"
        else
          echo "❌ Data validation failed after PVC restoration on ${{ matrix.architecture }}. Count: $count"
          exit 1
        fi

    - name: Cleanup PVC restored cluster port forwarding
      if: always()
      run: |
        # Stop port-forward if it exists
        if [ -f /tmp/pf_pid ]; then
          PF_PID=$(cat /tmp/pf_pid)
          kill $PF_PID 2>/dev/null || true
          rm -f /tmp/pf_pid
        fi
        
        # Clean up output log
        rm -f /tmp/pf_output.log
